{
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 9720860,
     "sourceType": "datasetVersion",
     "datasetId": 5947458
    },
    {
     "sourceId": 9730298,
     "sourceType": "datasetVersion",
     "datasetId": 5954512
    },
    {
     "sourceId": 9737132,
     "sourceType": "datasetVersion",
     "datasetId": 5959598
    },
    {
     "sourceId": 9737966,
     "sourceType": "datasetVersion",
     "datasetId": 5960234
    }
   ],
   "dockerImageVersionId": 30787,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "%matplotlib inline\n",
    "import json\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from torchvision import transforms\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from typing import Any, Tuple, List\n",
    "\n",
    "from cv2 import Mat\n",
    "from numpy import dtype, floating, integer, ndarray\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 10)  # (w, h)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-27T19:18:26.614424Z",
     "iopub.execute_input": "2024-10-27T19:18:26.614864Z",
     "iopub.status.idle": "2024-10-27T19:18:26.625103Z",
     "shell.execute_reply.started": "2024-10-27T19:18:26.614823Z",
     "shell.execute_reply": "2024-10-27T19:18:26.623948Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with open(\"kaggle/input/annotations/iwildcam2020_train_annotations.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "annotations = pd.DataFrame.from_dict(data[\"annotations\"])\n",
    "images_metadata = pd.DataFrame.from_dict(data[\"images\"])\n",
    "categories = pd.DataFrame.from_dict(data[\"categories\"])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-27T19:18:26.627190Z",
     "iopub.execute_input": "2024-10-27T19:18:26.627826Z",
     "iopub.status.idle": "2024-10-27T19:18:29.045494Z",
     "shell.execute_reply.started": "2024-10-27T19:18:26.627791Z",
     "shell.execute_reply": "2024-10-27T19:18:29.044630Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# convert datetime type and split into day/night time\n",
    "def split_day_night_time(\n",
    "    data: pd.DataFrame, day_start: str = \"06:00:00\", day_end: str = \"18:00:00\"\n",
    ") -> pd.DataFrame:\n",
    "    data = data.copy()\n",
    "    data[\"datetime\"] = pd.to_datetime(data[\"datetime\"])\n",
    "    data[\"is_day\"] = data[\"datetime\"].apply(\n",
    "        lambda x: True\n",
    "        if pd.Timestamp(day_start).time() <= x.time() < pd.Timestamp(day_end).time()\n",
    "        else False\n",
    "    )\n",
    "    return data"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-27T19:18:29.046762Z",
     "iopub.execute_input": "2024-10-27T19:18:29.047135Z",
     "iopub.status.idle": "2024-10-27T19:18:29.053763Z",
     "shell.execute_reply.started": "2024-10-27T19:18:29.047092Z",
     "shell.execute_reply": "2024-10-27T19:18:29.052824Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def preprocess_dark_images(\n",
    "    image: np.ndarray,\n",
    ") -> Mat | ndarray[Any, dtype[integer[Any] | floating[Any]]]:\n",
    "    img = cv2.cvtColor(image, cv2.COLOR_RGB2LUV)\n",
    "    img_eq = img.copy()\n",
    "    img_eq[:, :, 0] = cv2.equalizeHist(img[:, :, 0])\n",
    "    final_img = cv2.cvtColor(img_eq, cv2.COLOR_LUV2RGB)\n",
    "    return final_img"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-27T19:18:29.055181Z",
     "iopub.execute_input": "2024-10-27T19:18:29.055576Z",
     "iopub.status.idle": "2024-10-27T19:18:29.068001Z",
     "shell.execute_reply.started": "2024-10-27T19:18:29.055532Z",
     "shell.execute_reply": "2024-10-27T19:18:29.067253Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "class iWildCam2020Dataset(IterableDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: str,\n",
    "        metadata: pd.DataFrame,\n",
    "        batch_size: int = 16,\n",
    "        resize_dim: Tuple[int, int] | None = None,\n",
    "        num_samples: int = 1000,\n",
    "        mean: np.ndarray | None = None,\n",
    "        std: np.ndarray | None = None,\n",
    "        save_dir: str | None = None,\n",
    "        overwrite: bool = False,\n",
    "        split: str = \"train\",\n",
    "        val_ratio: float = 0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.metadata = metadata\n",
    "\n",
    "        self.split = split\n",
    "        self.val_ratio = val_ratio\n",
    "        self.train_size = int((1 - val_ratio) * num_samples)\n",
    "        self.val_size = num_samples - self.train_size\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.resize_dim = resize_dim\n",
    "\n",
    "        self.num_samples = num_samples\n",
    "        if self.split == \"train\":\n",
    "            self.num_batches = (self.train_size + batch_size - 1) // batch_size\n",
    "        else:\n",
    "            self.num_batches= (self.val_size + batch_size - 1) // batch_size\n",
    "\n",
    "        self.mean = torch.tensor(mean if mean is not None else [0.0, 0.0, 0.0]).view(\n",
    "            3, 1, 1\n",
    "        )\n",
    "        self.std = torch.tensor(std if std is not None else [1.0, 1.0, 1.0]).view(\n",
    "            3, 1, 1\n",
    "        )\n",
    "\n",
    "        self.save_dir = Path(save_dir) if save_dir else None\n",
    "        if self.save_dir:\n",
    "            self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.overwrite = overwrite\n",
    "\n",
    "    def save_image(self, img_tensor: torch.Tensor, idx: int):\n",
    "        if self.save_dir:\n",
    "            save_path = self.save_dir / f\"image_{idx}.pt\"\n",
    "            torch.save(img_tensor, save_path)\n",
    "    \n",
    "    def load_image(self, idx: int) -> torch.Tensor | None:\n",
    "        if self.save_dir:\n",
    "            save_path = self.save_dir / f\"image_{idx}.pt\"\n",
    "            if save_path.exists():\n",
    "                return torch.load(save_path, weights_only=True)\n",
    "        return None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.split == \"train\":\n",
    "            start_idx, end_idx = 0, self.train_size\n",
    "        else:\n",
    "            start_idx, end_idx = self.train_size, self.num_samples\n",
    "        \n",
    "        for idx, image_batch in enumerate(self.dataset.iter(self.batch_size)):\n",
    "            # to get consistent part of dataset + val / train split\n",
    "            batch_start = idx * self.batch_size\n",
    "            if batch_start >= end_idx:\n",
    "                break\n",
    "            if batch_start < start_idx:\n",
    "                continue\n",
    "            \n",
    "            is_day = self.metadata[idx * self.batch_size : (idx + 1) * self.batch_size][\n",
    "                \"is_day\"\n",
    "            ].values\n",
    "            image_batch = image_batch[\"image\"]\n",
    "            imgs_ = []\n",
    "\n",
    "            dark_idx = set(np.where(~is_day)[0].tolist())\n",
    "            for i in range(len(image_batch)):\n",
    "                img_tensor = self.load_image(idx * self.batch_size + i)\n",
    "                if img_tensor is None:\n",
    "                    img = np.transpose(image_batch[i].numpy())\n",
    "                    if i in dark_idx:\n",
    "                        img = preprocess_dark_images(img)\n",
    "                    img = cv2.resize(img, self.resize_dim, interpolation=cv2.INTER_AREA)\n",
    "                    img_tensor = (\n",
    "                        torch.tensor(np.transpose(img, (2, 0, 1)), dtype=torch.float32)\n",
    "                        / 255.0\n",
    "                    )\n",
    "\n",
    "                    if self.save_dir:\n",
    "                        self.save_image(img_tensor, idx * self.batch_size + i)\n",
    "\n",
    "                imgs_.append(img_tensor)\n",
    "            yield torch.stack(imgs_)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-27T19:18:29.070260Z",
     "iopub.execute_input": "2024-10-27T19:18:29.070594Z",
     "iopub.status.idle": "2024-10-27T19:18:29.090696Z",
     "shell.execute_reply.started": "2024-10-27T19:18:29.070562Z",
     "shell.execute_reply": "2024-10-27T19:18:29.089844Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def calculate_mean_std(dataset, batch_size=32, resize_dim=(224, 224), num_samples=1000):\n",
    "    means = []\n",
    "    stds = []\n",
    "    for idx, image_batch in tqdm(enumerate(dataset.iter(batch_size)), total = ((num_samples + batch_size - 1) // batch_size)):\n",
    "        if idx * batch_size >= num_samples:\n",
    "            break\n",
    "\n",
    "        imgs_ = []\n",
    "        for image in image_batch[\"image\"]:\n",
    "            img = np.transpose(image.numpy(), (1, 2, 0))\n",
    "            img = cv2.resize(img, resize_dim, interpolation=cv2.INTER_AREA)\n",
    "            img = img / 255.0\n",
    "            imgs_.append(img)\n",
    "\n",
    "        imgs_array = np.stack(imgs_)\n",
    "        means.append(imgs_array.mean(axis=(0, 1, 2)))\n",
    "        stds.append(imgs_array.std(axis=(0, 1, 2)))\n",
    "\n",
    "    mean = np.mean(means, axis=0)\n",
    "    std = np.mean(stds, axis=0)\n",
    "    return mean, std"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-27T19:18:29.091855Z",
     "iopub.execute_input": "2024-10-27T19:18:29.092544Z",
     "iopub.status.idle": "2024-10-27T19:18:29.108672Z",
     "shell.execute_reply.started": "2024-10-27T19:18:29.092499Z",
     "shell.execute_reply": "2024-10-27T19:18:29.107937Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    batch_size,\n",
    "    device,\n",
    "    num_epochs=1,\n",
    "    ckpt_path=\"best.pt\"\n",
    "):\n",
    "    best = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loop = tqdm(\n",
    "            enumerate(train_loader, 0),\n",
    "            total=len(train_loader),\n",
    "            desc=f\"Epoch {epoch}: train\",\n",
    "        )\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for i, batch in train_loop:\n",
    "            images = batch.to(device)\n",
    "            labels = torch.tensor(\n",
    "                annotations[\"category_id\"][\n",
    "                    epoch * (len(train_loader) * batch_size) + batch_size * i : min(\n",
    "                        epoch * (len(train_loader) * batch_size) + batch_size * (i + 1),\n",
    "                        len(annotations[\"category_id\"]),\n",
    "                    )\n",
    "                ].values\n",
    "            ).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs.to(device), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_loop.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            val_loop = tqdm(\n",
    "                enumerate(val_loader, 0),\n",
    "                total=len(val_loader),\n",
    "                desc=f\"Val\",\n",
    "            )\n",
    "\n",
    "            for i, batch in val_loop:\n",
    "                images = batch.to(device)\n",
    "                labels = torch.tensor(\n",
    "                    annotations[\"category_id\"][\n",
    "                        epoch * len(train_loader) * batch_size\n",
    "                        + batch_size * i : min(\n",
    "                            epoch * len(train_loader) * batch_size\n",
    "                            + batch_size * (i + 1),\n",
    "                            len(annotations[\"category_id\"]),\n",
    "                        )\n",
    "                    ].values\n",
    "                ).to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "                loss = criterion(outputs.to(device), labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                val_loop.set_postfix({\"acc\": correct / total, \"loss\": val_loss / (i + 1)})\n",
    "\n",
    "\n",
    "            val_accuracy = correct / total\n",
    "\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Training Loss: {train_loss / len(train_loader):.6f}\")\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Validation Accuracy: {val_accuracy:.6f}, Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "            if val_accuracy > best:\n",
    "                torch.save(model.state_dict(), ckpt_path)\n",
    "                best = correct / total"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-27T19:18:29.110087Z",
     "iopub.execute_input": "2024-10-27T19:18:29.110470Z",
     "iopub.status.idle": "2024-10-27T19:18:29.128443Z",
     "shell.execute_reply.started": "2024-10-27T19:18:29.110426Z",
     "shell.execute_reply": "2024-10-27T19:18:29.127577Z"
    },
    "trusted": true
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = load_dataset(\n",
    "    \"anngrosha/iWildCam2020\", split=\"train\", streaming=True\n",
    ").with_format(\"torch\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-27T19:18:29.129587Z",
     "iopub.execute_input": "2024-10-27T19:18:29.129944Z",
     "iopub.status.idle": "2024-10-27T19:18:30.388617Z",
     "shell.execute_reply.started": "2024-10-27T19:18:29.129901Z",
     "shell.execute_reply": "2024-10-27T19:18:30.387542Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "Resolving data files:   0%|          | 0/190 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f315af8b25364e24bef0e60fd7c52529"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Resolving data files:   0%|          | 0/52 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bcc891ac375047069e8e43efdebd41a7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Resolving data files:   0%|          | 0/190 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d7df7081ed3b48b58cc0fa15299c339f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Resolving data files:   0%|          | 0/52 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "336ad64475654636afc06312923a0991"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "images_metadata = split_day_night_time(images_metadata)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-27T19:18:30.390218Z",
     "iopub.execute_input": "2024-10-27T19:18:30.390920Z",
     "iopub.status.idle": "2024-10-27T19:18:49.083626Z",
     "shell.execute_reply.started": "2024-10-27T19:18:30.390867Z",
     "shell.execute_reply": "2024-10-27T19:18:49.082793Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Performing singe-batch overfitting to see if model capable enought for our task"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 5\n",
    "img_size = 640\n",
    "resize_dim = (img_size, img_size)\n",
    "num_classes = max(annotations[\"category_id\"])\n",
    "\n",
    "num_samples = 5000\n",
    "val_ratio = 0.1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-27T19:18:51.894210Z",
     "iopub.execute_input": "2024-10-27T19:18:51.894515Z",
     "iopub.status.idle": "2024-10-27T19:18:51.928268Z",
     "shell.execute_reply.started": "2024-10-27T19:18:51.894482Z",
     "shell.execute_reply": "2024-10-27T19:18:51.927295Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "mean, std = calculate_mean_std(\n",
    "    dataset, batch_size=batch_size, resize_dim=resize_dim, num_samples=num_samples\n",
    ")\n",
    "mean, std"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-27T19:18:51.929491Z",
     "iopub.execute_input": "2024-10-27T19:18:51.929785Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3349fe2a6f3b45309aaec4a28bd193ba"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "(array([0.38960105, 0.39468021, 0.36564374]),\n array([0.25637901, 0.25818906, 0.26048803]))"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset = iWildCam2020Dataset(\n",
    "    dataset=dataset,\n",
    "    metadata=images_metadata,\n",
    "    batch_size=batch_size,\n",
    "    resize_dim=resize_dim,\n",
    "    num_samples=num_samples,\n",
    "    mean=mean,\n",
    "    std=std,\n",
    "    save_dir=\"working/data/train\",\n",
    "    split=\"train\",\n",
    "    val_ratio=val_ratio\n",
    ")\n",
    "\n",
    "val_dataset = iWildCam2020Dataset(\n",
    "    dataset=dataset,\n",
    "    metadata=images_metadata,\n",
    "    batch_size=batch_size,\n",
    "    resize_dim=resize_dim,\n",
    "    num_samples=num_samples,\n",
    "    mean=mean,\n",
    "    std=std,\n",
    "    save_dir=\"working/data/val\",\n",
    "    split=\"val\",\n",
    "    val_ratio=val_ratio\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=None)\n",
    "val_loader = DataLoader(val_dataset, batch_size=None)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from IPython.display import clear_output"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install ultralytics\n",
    "!pip install -U ipywidgets\n",
    "clear_output()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is baseline YOLO to compare our trained model to the SOTA model as YOLO (YOLO could be used for classification, so this mode is used). \n",
    "\n",
    "Basic issues:\n",
    "1. YOLO trained on COCO dataset with general animal classes (we get them by hand down in the code + tried to make auto-mapping)\n",
    "2. For now we will judge YOLO if it finds animals (general classes) in the images (same data chunks as other model(s)), so this is easier task (12 classes vs. 267 classes)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolo11n-cls.yaml\").load(\"yolo11n-cls.pt\").to(device)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/Users/diazzz/Library/Application Support/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "YOLO11n-cls summary: 151 layers, 1,633,584 parameters, 1,633,584 gradients, 3.3 GFLOPs\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-cls.pt to 'yolo11n-cls.pt'...\n",
      "⚠️ Download failure, retrying 1/3 https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-cls.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "######################################################################## 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred 234/236 items from pretrained weights\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model = torch.load(\"yolo11n-cls.pt\")[\"model\"].to(device)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "model.float()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1, rho=0.95, weight_decay=1e-5)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    batch_size,\n",
    "    device,\n",
    "    num_epochs=5\n",
    ")"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "Epoch 0: train:   0%|          | 0/900 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "10bcf27971b24f7db46dd1e68f67a7d5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Val:   0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d8ee9d43d5444951bcaeb52377fa6371"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Training Loss: 1.316656\n",
      "Epoch [1/5], Validation Accuracy: 0.250000, Validation Loss: 682.500370\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 1: train:   0%|          | 0/900 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f16fb9c651c8425798aeb7584eec271e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Val:   0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d0dcae8c49d4021a95b5201b94d2dba"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Training Loss: 1.197736\n",
      "Epoch [2/5], Validation Accuracy: 0.000000, Validation Loss: 690.946805\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 2: train:   0%|          | 0/900 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "281ab65bed344f0bbc6a08ba39d97241"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Val:   0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "654b186be81d45eb82a48d0630ae416f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Training Loss: 1.297397\n",
      "Epoch [3/5], Validation Accuracy: 0.142000, Validation Loss: 676.848558\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 3: train:   0%|          | 0/900 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7a52d41288594a278ca284e6340628b5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Val:   0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51b7f73b876646cfa91e05122f5bcbce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Training Loss: 0.515178\n",
      "Epoch [4/5], Validation Accuracy: 0.000000, Validation Loss: 690.676773\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 4: train:   0%|          | 0/900 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "590eb70f363649448f9069167c187b65"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Val:   0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ebe9b94b4a6c4799a56af68a96ae3615"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Training Loss: 0.824677\n",
      "Epoch [5/5], Validation Accuracy: 0.036000, Validation Loss: 688.079607\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mshinbayeva\u001B[0m (\u001B[33mshinbayeva-shinbayeva\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.18.5"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/Users/diazzz/PycharmProjects/mlaru/wandb/run-20241027_233440-30cmmh5y</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/shinbayeva-shinbayeva/yolov11n-training-v1/runs/30cmmh5y' target=\"_blank\">sunny-meadow-1</a></strong> to <a href='https://wandb.ai/shinbayeva-shinbayeva/yolov11n-training-v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/shinbayeva-shinbayeva/yolov11n-training-v1' target=\"_blank\">https://wandb.ai/shinbayeva-shinbayeva/yolov11n-training-v1</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/shinbayeva-shinbayeva/yolov11n-training-v1/runs/30cmmh5y' target=\"_blank\">https://wandb.ai/shinbayeva-shinbayeva/yolov11n-training-v1/runs/30cmmh5y</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f021bb127dcd4a8db9b8db0205e09c94"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▆▁▄▁█</td></tr><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train_loss</td><td>█▇█▁▄</td></tr><tr><td>val_loss</td><td>▄█▁█▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.36</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>train_loss</td><td>0.82468</td></tr><tr><td>val_loss</td><td>688.0797</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">sunny-meadow-1</strong> at: <a href='https://wandb.ai/shinbayeva-shinbayeva/yolov11n-training-v1/runs/30cmmh5y' target=\"_blank\">https://wandb.ai/shinbayeva-shinbayeva/yolov11n-training-v1/runs/30cmmh5y</a><br/> View project at: <a href='https://wandb.ai/shinbayeva-shinbayeva/yolov11n-training-v1' target=\"_blank\">https://wandb.ai/shinbayeva-shinbayeva/yolov11n-training-v1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20241027_233440-30cmmh5y/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "num_epochs = 5\n",
    "\n",
    "# Инициализация сессии W&B\n",
    "wandb.init(project=\"yolov11n-training-v1\", config={\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 5,\n",
    "})\n",
    "\n",
    "wandb.log({'epoch': 1, 'train_loss': 1.316656, 'val_loss': 682.500370, 'accuracy': 0.25})\n",
    "wandb.log({'epoch': 2, 'train_loss': 1.197736, 'val_loss': 690.946805, 'accuracy': 0})\n",
    "wandb.log({'epoch': 3, 'train_loss': 1.297397, 'val_loss': 676.848558, 'accuracy': 0.142})\n",
    "wandb.log({'epoch': 4, 'train_loss': 0.515178, 'val_loss': 690.676773, 'accuracy': 0})\n",
    "wandb.log({'epoch': 5, 'train_loss': 0.824677, 'val_loss': 688.079697, 'accuracy': 0.36})\n",
    "\n",
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "iwildcam_class_names = categories[\"name\"].values.tolist()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import yaml\n",
    "\n",
    "def load_yolo_classes(yolo_yaml_path):\n",
    "    with open(yolo_yaml_path, 'r') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    return data['names'] \n",
    "\n",
    "yolo_classes = load_yolo_classes(\"kaggle/input/yolo-coco-names/yolo_coco.yaml\")"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class_mapping = {}\n",
    "\n",
    "for yolo_id, yolo_class in yolo_classes.items():\n",
    "    if yolo_class in iwildcam_class_names:\n",
    "        iwildcam_index = iwildcam_class_names.index(yolo_class)\n",
    "        class_mapping[yolo_id] = iwildcam_index\n",
    "        print(yolo_class)\n",
    "\n",
    "print(\"Class Mapping:\")\n",
    "print(class_mapping)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty\n",
      "tayassu pecari\n",
      "dasyprocta punctata\n",
      "cuniculus paca\n",
      "puma concolor\n",
      "tapirus terrestris\n",
      "pecari tajacu\n",
      "mazama americana\n",
      "leopardus pardalis\n",
      "geotrygon montana\n",
      "nasua nasua\n",
      "dasypus novemcinctus\n",
      "eira barbara\n",
      "didelphis marsupialis\n",
      "penelope jacquacu\n",
      "procyon cancrivorus\n",
      "aramides cajaneus\n",
      "panthera onca\n",
      "myrmecophaga tridactyla\n",
      "tinamus major\n",
      "crypturellus sp\n",
      "sylvilagus brasiliensis\n",
      "priodontes maximus\n",
      "tamandua tetradactyla\n",
      "tigrisoma lineatum\n",
      "cochlearius cochlearius\n",
      "puma yagouaroundi\n",
      "leopardus wiedii\n",
      "buteogallus urubitinga\n",
      "mazama gouazoubira\n",
      "philander opossum\n",
      "capra aegagrus\n",
      "bos taurus\n",
      "ovis aries\n",
      "canis lupus\n",
      "lepus saxatilis\n",
      "human\n",
      "turtur calcospilos\n",
      "papio anubis\n",
      "unknown\n",
      "genetta genetta\n",
      "tragelaphus scriptus\n",
      "equus africanus\n",
      "herpestes sanguineus\n",
      "loxodonta africana\n",
      "cricetomys gambianus\n",
      "raphicerus campestris\n",
      "hyaena hyaena\n",
      "aepyceros melampus\n",
      "crocuta crocuta\n",
      "caracal caracal\n",
      "equus ferus\n",
      "panthera leo\n",
      "tragelaphus oryx\n",
      "kobus ellipsiprymnus\n",
      "phacochoerus africanus\n",
      "panthera pardus\n",
      "ichneumia albicauda\n",
      "canis mesomelas\n",
      "xerus erythropus\n",
      "syncerus caffer\n",
      "equus quagga\n",
      "giraffa camelopardalis\n",
      "alcelaphus buselaphus\n",
      "chlorocebus pygerythrus\n",
      "madoqua guentheri\n",
      "potamochoerus larvatus\n",
      "nanger granti\n",
      "eudorcas thomsonii\n",
      "struthio camelus\n",
      "orycteropus afer\n",
      "acinonyx jubatus\n",
      "eupodotis senegalensis\n",
      "felis silvestris\n",
      "oryx beisa\n",
      "lophotis gindiana\n",
      "ardeotis kori\n",
      "helogale parvula\n",
      "lissotis melanogaster\n",
      "macaca nemestrina\n",
      "argusianus argus\n",
      "prionailurus bengalensis\n",
      "hemigalus derbyanus\n",
      "muntiacus muntjak\n",
      "sus scrofa\n",
      "helarctos malayanus\n",
      "rusa unicolor\n",
      "hystrix brachyura\n",
      "pardofelis temminckii\n",
      "panthera tigris\n",
      "lariscus insignis\n",
      "chalcophaps indica\n",
      "genetta tigrina\n",
      "hystrix cristata\n",
      "lycaon pictus\n",
      "procavia capensis\n",
      "streptopilia senegalensis\n",
      "ictonyx striatus\n",
      "canis familiaris\n",
      "unknown bird\n",
      "leptotila rufaxilla\n",
      "chelonoidis carbonaria\n",
      "unknown bat\n",
      "crypturellus soui\n",
      "momotus momota\n",
      "psophia crepitans\n",
      "nothocrax urumutum\n",
      "dasyprocta fuliginosa\n",
      "myoprocta pratti\n",
      "proechimys sp\n",
      "geotrygon sp\n",
      "nasua narica\n",
      "tamandua mexicana\n",
      "didelphis sp\n",
      "penelope purpurascens\n",
      "aguila sp\n",
      "phaetornis sp\n",
      "paleosuchus sp\n",
      "brotogeris sp\n",
      "camelus dromedarius\n",
      "otocyon megalotis\n",
      "acryllium vulturinum\n",
      "equus grevyi\n",
      "proteles cristata\n",
      "leptailurus serval\n",
      "tragelaphus strepsiceros\n",
      "hippopotamus amphibius\n",
      "burhinus capensis\n",
      "tockus deckeni\n",
      "xerus rutilus\n",
      "unidentifiable\n",
      "paguma larvata\n",
      "pardofelis marmorata\n",
      "cuon alpinus\n",
      "varanus salvator\n",
      "martes flavigula\n",
      "prionodon linsang\n",
      "rollulus rouloul\n",
      "lophura inornata\n",
      "polyplectron chalcurum\n",
      "manis javanica\n",
      "capricornis sumatraensis\n",
      "macaca fascicularis\n",
      "unknown squirrel\n",
      "francolinus nobilis\n",
      "cercopithecus lhoesti\n",
      "cephalophus nigrifrons\n",
      "atherurus africanus\n",
      "pan troglodytes\n",
      "cercopithecus mitis\n",
      "funisciurus carruthersi\n",
      "motacilla flava\n",
      "andropadus latirostris\n",
      "andropadus virens\n",
      "thamnomys venustus\n",
      "protoxerus stangeri\n",
      "paraxerus boehmi\n",
      "cephalophus silvicultor\n",
      "melaenornis ardesiacus\n",
      "oenomys hypoxanthus\n",
      "nectarinia cyanolaema\n",
      "nesocharis capistrata\n",
      "melocichla mentalis\n",
      "hybomys univittatus\n",
      "colomys goslingi\n",
      "hylomyscus stella\n",
      "genetta servalina\n",
      "canis adustus\n",
      "dioptrornis fischeri\n",
      "mus minutoides\n",
      "andropadus gracilirostris\n",
      "musophaga rossae\n",
      "acrocephalus baeticatus\n",
      "turtur tympanistria\n",
      "praomys tullbergi\n",
      "malacomys longipes\n",
      "start\n",
      "end\n",
      "eurocephalus rueppelli\n",
      "alopochen aegyptiaca\n",
      "deomys ferrugineus\n",
      "francolinus africanus\n",
      "nandinia binotata\n",
      "mesopicos griseocephalus\n",
      "turdus olivaceus\n",
      "streptopelia lugens\n",
      "mazama sp\n",
      "urocyon cinereoargenteus\n",
      "meleagris ocellata\n",
      "crax rubra\n",
      "agouti paca\n",
      "tapirus bairdii\n",
      "procyon lotor\n",
      "odocoileus virginianus\n",
      "leptotila plumbeiceps\n",
      "mazama temama\n",
      "conepatus semistriatus\n",
      "mazama pandora\n",
      "ortalis vetula\n",
      "presbytis thomasi\n",
      "neofelis diardi\n",
      "arctonyx hoevenii\n",
      "tragulus sp\n",
      "arctictis binturong\n",
      "misfire\n",
      "dendrocitta occipitalis\n",
      "niltava sumatrana\n",
      "mustela lutreolina\n",
      "leiothrix argentauris\n",
      "myiophoneus melanurus\n",
      "arborophila rubrirostris\n",
      "sundasciurus hippurus\n",
      "erithacus cyane\n",
      "lophura sp\n",
      "myiophoneus glaucinus\n",
      "lophura erythrophthalma\n",
      "spilornis cheela\n",
      "myiophoneus caeruleus\n",
      "herpestes semitorquatus\n",
      "collocalia linchi\n",
      "callosciurus notatus\n",
      "cerdocyon thous\n",
      "unknown rat\n",
      "motorcycle\n",
      "peromyscus sp\n",
      "buteogallus urobitinga\n",
      "unknown raptor\n",
      "puma yagoroundi\n",
      "tigrisoma mexicanum\n",
      "canis latrans\n",
      "claravis pretiosa\n",
      "sciurus sp\n",
      "ave desconocida\n",
      "aramides cajanea\n",
      "unknown dove\n",
      "aramus guarauna\n",
      "mazama  temama\n",
      "unknown mouse or rat\n",
      "dasyprocta leporina\n",
      "crax alector\n",
      "didelphis imperfecta\n",
      "unknown opossum\n",
      "mesembrinis cayannensis\n",
      "myoprocta pratii\n",
      "metachirus nudicudatus\n",
      "unknown armadillo\n",
      "mazama gouazaoubira\n",
      "unknown cervid\n",
      "catharus ustulatus\n",
      "leopardus weidii\n",
      "tayassu tajacu\n",
      "mitu tomentosa\n",
      "cabassous unicintus\n",
      "puma yaguaroundi\n",
      "crypturellus variegatus\n",
      "unknown pig_peccary\n",
      "neomorphus rufipennis\n",
      "pipile pipile\n",
      "metachirus nudicaudatus\n",
      "funisciurus pyrropus\n",
      "alethe poliophrys\n",
      "ruwenzorornis johnstoni\n",
      "thryonomys swinderianus\n",
      "ploceus alienus\n",
      "ploceus baglafecht\n",
      "poecilogale albinucha\n",
      "anomalurus derbianus\n",
      "Class Mapping:\n",
      "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29, 30: 30, 31: 31, 32: 32, 33: 33, 34: 34, 35: 35, 36: 36, 37: 37, 38: 38, 39: 39, 40: 40, 41: 41, 42: 42, 43: 43, 44: 44, 45: 45, 46: 46, 47: 47, 48: 48, 49: 49, 50: 50, 51: 51, 52: 52, 53: 53, 54: 54, 55: 55, 56: 56, 57: 57, 58: 58, 59: 59, 60: 60, 61: 61, 62: 62, 63: 63, 64: 64, 65: 65, 66: 66, 67: 67, 68: 68, 69: 69, 70: 70, 71: 71, 72: 72, 73: 73, 74: 74, 75: 75, 76: 76, 77: 77, 78: 78, 79: 79, 80: 80, 81: 81, 82: 82, 83: 83, 84: 84, 85: 85, 86: 86, 87: 87, 88: 88, 89: 89, 90: 90, 91: 91, 92: 92, 93: 93, 94: 94, 95: 95, 96: 96, 97: 97, 98: 98, 99: 99, 100: 100, 101: 101, 102: 102, 103: 103, 104: 104, 105: 105, 106: 106, 107: 107, 108: 108, 109: 109, 110: 110, 111: 111, 112: 112, 113: 113, 114: 114, 115: 115, 116: 116, 117: 117, 118: 118, 119: 119, 120: 120, 121: 121, 122: 122, 123: 123, 124: 124, 125: 125, 126: 126, 127: 127, 128: 128, 129: 129, 130: 130, 131: 131, 132: 132, 133: 133, 134: 134, 135: 135, 136: 136, 137: 137, 138: 138, 139: 139, 140: 140, 141: 141, 142: 142, 143: 143, 144: 144, 145: 145, 146: 146, 147: 147, 148: 148, 149: 149, 150: 150, 151: 151, 152: 152, 153: 153, 154: 154, 155: 155, 156: 156, 157: 157, 158: 158, 159: 159, 160: 160, 161: 161, 162: 162, 163: 163, 164: 164, 165: 165, 166: 166, 167: 167, 168: 168, 169: 169, 170: 170, 171: 171, 172: 172, 173: 173, 174: 174, 175: 175, 176: 176, 177: 177, 178: 178, 179: 179, 180: 180, 181: 181, 182: 182, 183: 183, 184: 184, 185: 185, 186: 186, 187: 187, 188: 188, 189: 189, 190: 190, 191: 191, 192: 192, 193: 193, 194: 194, 195: 195, 196: 196, 197: 197, 198: 198, 199: 199, 200: 200, 201: 201, 202: 202, 203: 203, 204: 204, 205: 205, 206: 206, 207: 207, 208: 208, 209: 209, 210: 210, 211: 211, 212: 212, 213: 213, 214: 214, 215: 215, 216: 216, 217: 217, 218: 218, 219: 219, 220: 220, 221: 221, 222: 222, 223: 223, 224: 224, 225: 225, 226: 226, 227: 227, 228: 228, 229: 229, 230: 230, 231: 231, 232: 232, 233: 233, 234: 234, 235: 235, 236: 236, 237: 237, 238: 238, 239: 239, 240: 240, 241: 241, 242: 242, 243: 243, 244: 244, 245: 245, 246: 246, 247: 247, 248: 248, 249: 249, 250: 250, 251: 251, 252: 252, 253: 253, 254: 254, 255: 255, 256: 256, 257: 257, 258: 258, 259: 259, 260: 260, 261: 261, 262: 262, 263: 263, 264: 264, 265: 265, 266: 266}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "coco_idx = [0, 3, 15, 16, 17, 18, 19, 20, 21, 22, 23]"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def count_valid_classes(model, dataloader, coco_animal_idx, device):\n",
    "    counts = {idx: 0 for idx in coco_animal_idx}\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        main_loop = tqdm(\n",
    "            enumerate(dataloader, 0),\n",
    "            total=len(dataloader),\n",
    "            desc=f\"Evaluating\",\n",
    "        )\n",
    "        \n",
    "        for i, images in main_loop:\n",
    "            print(\"Batch index:\", i)\n",
    "            images = images.to(device)\n",
    "            results = model.predict(images, augment=False, save=False, verbose=False, device=device)\n",
    "            \n",
    "            for i, result in enumerate(results):\n",
    "                probs = result.probs.data\n",
    "\n",
    "                predicted_class = torch.argmax(probs).item()\n",
    "                total += 1\n",
    "                \n",
    "                if predicted_class in coco_animal_idx:\n",
    "                    counts[predicted_class] += 1\n",
    "                    correct += 1\n",
    "            \n",
    "            main_loop.set_postfix({\"acc\": correct / total})\n",
    "\n",
    "    return counts, correct / total"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_yolo = YOLO(\"yolo11n-cls.yaml\").load(\"yolo11n-cls.pt\").to(device)\n",
    "valid_counts, acc = count_valid_classes(model_yolo, val_loader, coco_idx, device)\n",
    "print(acc)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}

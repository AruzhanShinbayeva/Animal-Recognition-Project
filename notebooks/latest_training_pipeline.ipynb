{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import json\n",
    "from typing import Any, Tuple\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from cv2 import Mat\n",
    "from datasets import load_dataset\n",
    "from numpy import dtype, floating, integer, ndarray\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler, Subset\n",
    "from tqdm.autonotebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 10)  # (w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ilia/micromamba/envs/project-pmldl/lib/python3.11/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "# Load Hugging Face dataset\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "    \"microsoft/swinv2-base-patch4-window16-256\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/iwildcam2020_train_annotations.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "annotations = pd.DataFrame.from_dict(data[\"annotations\"])\n",
    "images_metadata = pd.DataFrame.from_dict(data[\"images\"])\n",
    "categories = pd.DataFrame.from_dict(data[\"categories\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datetime type and split into day/night time\n",
    "def split_day_night_time(\n",
    "    data: pd.DataFrame, day_start: str = \"06:00:00\", day_end: str = \"18:00:00\"\n",
    ") -> pd.DataFrame:\n",
    "    data = data.copy()\n",
    "    data[\"datetime\"] = pd.to_datetime(data[\"datetime\"])\n",
    "    data[\"is_day\"] = data[\"datetime\"].apply(\n",
    "        lambda x: True\n",
    "        if pd.Timestamp(day_start).time() <= x.time() < pd.Timestamp(day_end).time()\n",
    "        else False\n",
    "    )\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_dark_images(\n",
    "    image: np.ndarray,\n",
    ") -> Mat | ndarray[Any, dtype[integer[Any] | floating[Any]]]:\n",
    "    img = cv2.cvtColor(image, cv2.COLOR_RGB2LUV)\n",
    "    img_eq = img.copy()\n",
    "    img_eq[:, :, 0] = cv2.equalizeHist(img[:, :, 0])\n",
    "    final_img = cv2.cvtColor(img_eq, cv2.COLOR_LUV2RGB)\n",
    "    return final_img\n",
    "\n",
    "\n",
    "def crop_black_lines(image: np.ndarray) -> np.ndarray:\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    _, mask = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if contours:\n",
    "        x, y, w, h = cv2.boundingRect(contours[0])\n",
    "        cropped_image = image[y : y + h, x : x + w]\n",
    "        return cropped_image\n",
    "    else:\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import Error\n",
    "from pathlib import Path\n",
    "from itertools import islice\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "\n",
    "\n",
    "class iWildCam2020Preprocessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: str,\n",
    "        metadata: pd.DataFrame,\n",
    "        annotations,\n",
    "        batch_size: int = 1,\n",
    "        resize_dim: Tuple[int, int] | None = None,\n",
    "        num_samples: int = 1000,\n",
    "        save_dir: str = \"./processed_images\",\n",
    "        overwrite: bool = False,\n",
    "    ):\n",
    "        self.metadata = metadata\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.resize_dim = resize_dim\n",
    "\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.overwrite = overwrite\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.annotations = annotations\n",
    "\n",
    "        unique_classes = self.annotations[\"category_id\"].unique()\n",
    "        category_to_index = {\n",
    "            category_id: index for index, category_id in enumerate(unique_classes)\n",
    "        }\n",
    "        self.annotations[\"mapped_category_id\"] = self.annotations[\"category_id\"].map(\n",
    "            category_to_index\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def is_valid(image: np.ndarray) -> bool:\n",
    "        if (\n",
    "            image.ndim not in [3, 4]\n",
    "            or image.shape[0] == 1\n",
    "            or image.shape[1] == 1\n",
    "            or image.shape[2] != 3\n",
    "        ):\n",
    "            print(f\"Skipping image with invalid shape: {image.shape}\")\n",
    "            return False, None\n",
    "\n",
    "        if image.ndim == 4:\n",
    "            image = np.squeeze(image, axis=-1)\n",
    "\n",
    "        if image.ndim == 3 and image.shape[2] == 3:\n",
    "            try:\n",
    "                img = Image.fromarray(image.astype(np.uint8), mode=\"RGB\")\n",
    "                img.verify()\n",
    "                img.load()\n",
    "                img = img.convert(\"RGB\")\n",
    "                return True, img\n",
    "            except (UnidentifiedImageError, IOError) as e:\n",
    "                print(f\"Error while processing RGB image: {e}\")\n",
    "                return False, None\n",
    "\n",
    "        return False, None\n",
    "\n",
    "    def preprocess_dataset(self):\n",
    "        existing_files = list(self.save_dir.glob(\"image_*.pt\"))\n",
    "        existing_files.sort(key=lambda x: int(x.stem.split(\"_\")[1]))\n",
    "\n",
    "        last_processed_index = (\n",
    "            int(existing_files[-1].stem.split(\"_\")[1]) if existing_files else 0\n",
    "        )\n",
    "        image_iterator = self.dataset.iter(batch_size=self.batch_size)\n",
    "        if last_processed_index != 0:\n",
    "            image_iterator = islice(\n",
    "                image_iterator, last_processed_index // self.batch_size\n",
    "            )\n",
    "\n",
    "        saved_samples = last_processed_index + 1 if last_processed_index != 0 else 0\n",
    "        idx = saved_samples\n",
    "        with tqdm(\n",
    "            total=self.num_samples,\n",
    "            initial=(last_processed_index + 1 if last_processed_index != 0 else 0),\n",
    "        ) as pbar:\n",
    "            while saved_samples < self.num_samples:\n",
    "                try:\n",
    "                    batch = next(image_iterator)\n",
    "                    for i, images in enumerate(batch[\"image\"]):\n",
    "                        save_path = self.save_dir / f\"image_{saved_samples}.pt\"\n",
    "\n",
    "                        if save_path.exists() and not self.overwrite:\n",
    "                            pbar.update(1)\n",
    "                            saved_samples += 1\n",
    "                            continue\n",
    "\n",
    "                        img_np = np.transpose(images.numpy())\n",
    "\n",
    "                        valid, img = self.is_valid(img_np)\n",
    "                        if not valid:\n",
    "                            print(\n",
    "                                f\"Skipping invalid or corrupt image at index {idx + i}, {img_np.shape}\"\n",
    "                            )\n",
    "                            pbar.update(0)\n",
    "                            continue\n",
    "\n",
    "                        img_np = np.array(img)\n",
    "\n",
    "                        is_day = self.metadata.iloc[idx + i][\"is_day\"]\n",
    "                        if not is_day:\n",
    "                            img_np = preprocess_dark_images(img_np)\n",
    "\n",
    "                        img_np = crop_black_lines(img_np)\n",
    "                        img_np = cv2.resize(\n",
    "                            img_np, self.resize_dim, interpolation=cv2.INTER_AREA\n",
    "                        )\n",
    "\n",
    "                        img_tensor = (\n",
    "                            torch.tensor(\n",
    "                                np.transpose(img_np, (2, 0, 1)), dtype=torch.float32\n",
    "                            )\n",
    "                            / 255.0\n",
    "                        )\n",
    "\n",
    "                        label = self.annotations.iloc[idx][\"mapped_category_id\"]\n",
    "                        data = {\n",
    "                            \"image\": img_tensor,\n",
    "                            \"label\": label,\n",
    "                        }\n",
    "\n",
    "                        if not save_path.exists() or self.overwrite:\n",
    "                            torch.save(data, save_path)\n",
    "\n",
    "                        saved_samples += 1\n",
    "                        pbar.update(1)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipping record at index {idx} due to error\")\n",
    "                    pbar.update(0)\n",
    "                idx += self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iWildCam2020Dataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        transform: transforms.Compose | None = None,\n",
    "        save_dir: str = \"./data/processed_images\",\n",
    "    ):\n",
    "        self.save_dir = Path(save_dir)\n",
    "\n",
    "        self.transform = transform\n",
    "        self.items = list(self.save_dir.glob(\"image_*.pt\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.save_dir / f\"image_{idx}.pt\"\n",
    "        data = torch.load(img_path)\n",
    "        \n",
    "        img_tensor = data[\"image\"]\n",
    "        label = data[\"label\"]\n",
    "\n",
    "        if self.transform:\n",
    "            img_tensor = self.transform(img_tensor)\n",
    "\n",
    "        return img_tensor, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def get_unique_model_path(base_path):\n",
    "    if not os.path.exists(base_path):\n",
    "        return base_path\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    unique_path = f\"{base_path}_{timestamp}.pt\"\n",
    "\n",
    "    while os.path.exists(unique_path):\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        unique_path = f\"{base_path}_{timestamp}.pt\"\n",
    "\n",
    "    return unique_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import evaluate\n",
    "\n",
    "\n",
    "def init_metrics() -> Dict[str, evaluate.Metric]:\n",
    "    return {\n",
    "        \"accuracy\": evaluate.load(\"accuracy\"),\n",
    "        \"precision\": evaluate.load(\"precision\", zero_division=0, average=\"macro\"),\n",
    "        \"recall\": evaluate.load(\"recall\", zero_division=0, average=\"macro\"),\n",
    "        \"f1\": evaluate.load(\"f1\", average=\"macro\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_batch_metrics(metrics: Dict[str, evaluate.Metric]) -> Dict[str, float]:\n",
    "    computed_metrics = {}\n",
    "\n",
    "    computed_metrics[\"accuracy\"] = metrics[\"accuracy\"].compute()[\"accuracy\"]\n",
    "    computed_metrics[\"precision\"] = metrics[\"precision\"].compute(\n",
    "        zero_division=0, average=\"macro\"\n",
    "    )[\"precision\"]\n",
    "    computed_metrics[\"recall\"] = metrics[\"recall\"].compute(\n",
    "        zero_division=0, average=\"macro\"\n",
    "    )[\"recall\"]\n",
    "    computed_metrics[\"f1\"] = metrics[\"f1\"].compute(average=\"macro\")[\"f1\"]\n",
    "\n",
    "    return computed_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import autocast\n",
    "\n",
    "# comment if cuda unavaliabe\n",
    "#from torch import GradScaler\n",
    "\n",
    "\n",
    "def train_with_lora_and_hard_negatives(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    batch_size,\n",
    "    num_samples,\n",
    "    device,\n",
    "    num_epochs=1,\n",
    "    ckpt_path=\"models/best.pt\",\n",
    "    use_mlflow=False,\n",
    "    use_wandb=False,\n",
    "    grad_clip_norm=None,\n",
    "    scheduler=None,\n",
    "    hard_negative_ratio=0.1,\n",
    "    hard_negative_update_freq=1,\n",
    "    use_amp=False,\n",
    "):\n",
    "    ckpt_path = get_unique_model_path(ckpt_path)\n",
    "    best_accuracy = 0.0\n",
    "    metrics = init_metrics()\n",
    "\n",
    "    hard_negatives = []\n",
    "\n",
    "    if use_mlflow:\n",
    "        import mlflow\n",
    "\n",
    "        mlflow.start_run()\n",
    "        mlflow.log_params(\n",
    "            {\n",
    "                \"model\": model.__class__.__name__,\n",
    "                \"criterion\": criterion.__class__.__name__,\n",
    "                \"optimizer\": optimizer.__class__.__name__,\n",
    "                \"num_epochs\": num_epochs,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"num_samples\": num_samples,\n",
    "                \"model_path\": ckpt_path,\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    # comment if cuda unavaliabe\n",
    "    #scaler = GradScaler() if use_amp else None\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        current_hard_negatives = []\n",
    "\n",
    "        if len(hard_negatives) > 0:\n",
    "            hard_negative_batch = generate_hard_negative_batch(hard_negatives, batch_size, device)\n",
    "            train_loader_with_hard_negatives = concatenate_batches(train_loader, hard_negative_batch, device)\n",
    "        else:\n",
    "            train_loader_with_hard_negatives = train_loader\n",
    "\n",
    "        for images, labels in tqdm(train_loader_with_hard_negatives, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                outputs = model(images)\n",
    "                ind_loss = torch.nn.functional.cross_entropy(outputs.logits, labels, reduction='none')\n",
    "                loss = ind_loss.mean()  # Mean loss for the batch\n",
    "\n",
    "            if use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "                if grad_clip_norm:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update() \n",
    "            else:\n",
    "                loss.backward()\n",
    "                if grad_clip_norm:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            misclassified = preds != labels\n",
    "            hard_negative_losses = ind_loss[misclassified]\n",
    "            current_hard_negatives.extend(\n",
    "                [(images[i], labels[i]) for i, _ in enumerate(hard_negative_losses)]\n",
    "            )\n",
    "\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Update hard negatives\n",
    "        if epoch % hard_negative_update_freq == 0:\n",
    "            hard_negatives.extend(current_hard_negatives)\n",
    "            max_negatives = int(hard_negative_ratio * len(train_loader.dataset))\n",
    "            hard_negatives = hard_negatives[-max_negatives:]\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        computed_metrics = {}\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                with autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs.logits, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = outputs.logits.argmax(dim=1)\n",
    "                metrics[\"accuracy\"].add_batch(predictions=preds, references=labels)\n",
    "                metrics[\"precision\"].add_batch(predictions=preds, references=labels)\n",
    "                metrics[\"recall\"].add_batch(predictions=preds, references=labels)\n",
    "                metrics[\"f1\"].add_batch(predictions=preds, references=labels)\n",
    "\n",
    "            computed_metrics = compute_batch_metrics(metrics=metrics)\n",
    "\n",
    "        # Log and save\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        log_data = {\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "            **computed_metrics,\n",
    "        }\n",
    "\n",
    "        if use_mlflow:\n",
    "            mlflow.log_metrics(log_data, step=epoch)\n",
    "        if use_wandb:\n",
    "            wandb.log(log_data)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\"\n",
    "        )\n",
    "        print(f\"Metrics: {computed_metrics}\")\n",
    "\n",
    "        if computed_metrics[\"accuracy\"] > best_accuracy:\n",
    "            best_accuracy = computed_metrics[\"accuracy\"]\n",
    "            torch.save(model.state_dict(), ckpt_path)\n",
    "            if use_mlflow:\n",
    "                mlflow.pytorch.log_model(model, ckpt_path)\n",
    "\n",
    "    if use_mlflow:\n",
    "        mlflow.end_run()\n",
    "\n",
    "def generate_hard_negative_batch(hard_negatives, batch_size, device):\n",
    "    if len(hard_negatives) < batch_size:\n",
    "        return DataLoader(hard_negatives, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    indices = torch.randint(0, len(hard_negatives), (batch_size,))\n",
    "    batch = [hard_negatives[i] for i in indices]\n",
    "    images, labels = zip(*batch)\n",
    "    return torch.stack(images).to(device), torch.stack(labels).to(device)\n",
    "\n",
    "def concatenate_batches(train_loader, hard_negative_batch, device):\n",
    "    train_images, train_labels = next(iter(train_loader))\n",
    "    train_images, train_labels = train_images.to(device), train_labels.to(device)\n",
    "\n",
    "    images, labels = hard_negative_batch\n",
    "    combined_images = torch.cat((train_images, images), dim=0)\n",
    "    combined_labels = torch.cat((train_labels, labels), dim=0)\n",
    "    combined_loader = torch.utils.data.DataLoader(\n",
    "        list(zip(combined_images, combined_labels)), batch_size=train_loader.batch_size, shuffle=True\n",
    "    )\n",
    "    return combined_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a3e4ab59d5045a9ae0d374ec3f229f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5420259102934eb0a534a85dd80287cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3992810e546e4f5da5e61592648370ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0535da6507ed46f1966227fc3ee8d169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    \"anngrosha/iWildCam2020\", split=\"train\", streaming=True\n",
    ").with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_metadata = split_day_night_time(images_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "img_size = 224\n",
    "resize_dim = (img_size, img_size)\n",
    "num_classes = len(annotations[\"category_id\"].unique())\n",
    "print(num_classes)\n",
    "\n",
    "num_samples = 20_000\n",
    "val_ratio = 0.2\n",
    "\n",
    "train_size = int(num_samples * (1 - val_ratio))\n",
    "val_size = int(num_samples * val_ratio)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "mean_std_samples = num_samples - int(num_samples * val_ratio)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"mps\"\n",
    "\n",
    "save_dir = \"./data/processed_images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009c0531818b46c3923725435224e425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "100%|##########| 20000/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_preprocessor = iWildCam2020Preprocessor(\n",
    "    dataset=dataset,\n",
    "    metadata=images_metadata,\n",
    "    resize_dim=resize_dim,\n",
    "    batch_size=100,\n",
    "    num_samples=num_samples,\n",
    "    save_dir=save_dir,\n",
    "    overwrite=False,\n",
    "    annotations=annotations,\n",
    ")\n",
    "dataset_preprocessor.preprocess_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2.9317e-09, 2.8943e-09, 2.8340e-09]),\n",
       " tensor([1.3166e-09, 1.3476e-09, 1.4205e-09]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_mean_std(\n",
    "    resize_dim=(224, 224),\n",
    "    num_samples=1000,\n",
    "    device=\"cpu\",\n",
    "    save_dir=\"./processed_images\",\n",
    "    save_files=True,\n",
    "):\n",
    "    total_pixels = 0\n",
    "    sum_mean = torch.zeros(3, dtype=torch.float32, device=device)\n",
    "    sum_std = torch.zeros(3, dtype=torch.float32, device=device)\n",
    "\n",
    "    image_files = list(Path(save_dir).glob(\"image_*.pt\"))\n",
    "    image_files.sort(key=lambda x: int(x.stem.split(\"_\")[1]))\n",
    "\n",
    "    image_files = image_files[:num_samples]\n",
    "\n",
    "    with tqdm(total=len(image_files)) as pbar:\n",
    "        for idx, image_file in enumerate(image_files):\n",
    "            img_tensor = torch.load(image_file)[\"image\"].to(device)\n",
    "\n",
    "            img_tensor = torch.nn.functional.interpolate(\n",
    "                img_tensor.unsqueeze(0),\n",
    "                size=resize_dim,\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False,\n",
    "            ).squeeze(0)\n",
    "\n",
    "            img_tensor = img_tensor / 255.0  # Normalize to [0, 1]\n",
    "            sum_mean += img_tensor.mean(dim=(1, 2))\n",
    "            sum_std += img_tensor.std(dim=(1, 2))\n",
    "            total_pixels += img_tensor.numel()\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    mean = sum_mean / total_pixels\n",
    "    std = sum_std / total_pixels\n",
    "\n",
    "    if save_files:\n",
    "        mean_file = Path(save_dir) / f\"mean_top_{num_samples}.pt\"\n",
    "        std_file = Path(save_dir) / f\"std_top_{num_samples}.pt\"\n",
    "        torch.save(mean, mean_file)\n",
    "        torch.save(std, std_file)\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "def get_mean_std_from_files(\n",
    "    save_dir=\"./processed_images\", num_samples=1000, device=\"cpu\"\n",
    "):\n",
    "    mean_file = Path(save_dir) / f\"mean_top_{num_samples}.pt\"\n",
    "    std_file = Path(save_dir) / f\"std_top_{num_samples}.pt\"\n",
    "\n",
    "    if mean_file.exists() and std_file.exists():\n",
    "        mean = torch.load(mean_file)\n",
    "        std = torch.load(std_file)\n",
    "        return mean, std\n",
    "    else:\n",
    "        return calculate_mean_std(\n",
    "            num_samples=num_samples, device=device, save_dir=save_dir, save_files=True\n",
    "        )\n",
    "\n",
    "\n",
    "mean, std = get_mean_std_from_files(save_dir, mean_std_samples, device=device)\n",
    "mean, std = mean.to(\"cpu\"), std.to(\"cpu\")\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomResizedCrop(size=(224, 224)),\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "        transforms.RandomApply([transforms.GaussianBlur(3)], p=0.1),\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = iWildCam2020Dataset(\n",
    "    save_dir=\"./data/processed_images\", transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "train_idx = list(range(train_size))\n",
    "val_idx = list(range(train_size, num_samples))\n",
    "\n",
    "train_dataset = Subset(dataset, train_idx)\n",
    "val_dataset = Subset(dataset, val_idx)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    \"microsoft/swinv2-tiny-patch4-window16-256\"\n",
    ")\n",
    "model.classifier = nn.Linear(768, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 378,072 || all params: 28,122,330 || trainable%: 1.3444\n"
     ]
    }
   ],
   "source": [
    "import peft\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.2,\n",
    "    target_modules=[\"query\", \"value\", \"key\"],\n",
    "    modules_to_save=[\"classifier\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c7d0bebe1348f4b5d16b55c2d6c912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/8000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1255d9306d41178a5512d34ba4571b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.0003, Val Loss: 0.0016\n",
      "Metrics: {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd841302f4334fab89168497374cdf37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9727c8672ad405ea3a407ddeb4b1a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Train Loss: 0.0003, Val Loss: 0.0016\n",
      "Metrics: {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fbbc75440984aea8cfc90695a21f0a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17006bc25ca0453883423d8cb3fb09c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_with_lora_and_hard_negatives\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/best-lora.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_clip_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhard_negative_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhard_negative_update_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 121\u001b[0m, in \u001b[0;36mtrain_with_lora_and_hard_negatives\u001b[0;34m(model, criterion, optimizer, train_loader, val_loader, batch_size, num_samples, device, num_epochs, ckpt_path, use_mlflow, use_wandb, grad_clip_norm, scheduler, hard_negative_ratio, hard_negative_update_freq, use_amp)\u001b[0m\n\u001b[1;32m    119\u001b[0m metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39madd_batch(predictions\u001b[38;5;241m=\u001b[39mpreds, references\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[1;32m    120\u001b[0m metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39madd_batch(predictions\u001b[38;5;241m=\u001b[39mpreds, references\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[0;32m--> 121\u001b[0m \u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39madd_batch(predictions\u001b[38;5;241m=\u001b[39mpreds, references\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/project-pmldl/lib/python3.11/site-packages/evaluate/module.py:515\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_feature_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_feature_from_batch(batch)\n\u001b[0;32m--> 515\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, column \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/micromamba/envs/project-pmldl/lib/python3.11/site-packages/evaluate/module.py:649\u001b[0m, in \u001b[0;36mEvaluationModule._init_writer\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;66;03m# Get cache file name and lock it\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilelock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 649\u001b[0m     cache_file_name, filelock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_cache_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# get ready\u001b[39;00m\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m cache_file_name\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilelock \u001b[38;5;241m=\u001b[39m filelock\n",
      "File \u001b[0;32m~/micromamba/envs/project-pmldl/lib/python3.11/site-packages/evaluate/module.py:284\u001b[0m, in \u001b[0;36mEvaluationModule._create_cache_file\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    282\u001b[0m filelock \u001b[38;5;241m=\u001b[39m FileLock(file_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.lock\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     \u001b[43mfilelock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Timeout:\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# If we have reached the max number of attempts or we are not allow to find a free name (distributed setup)\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# We raise an error\u001b[39;00m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_process \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/micromamba/envs/project-pmldl/lib/python3.11/site-packages/filelock/_api.py:344\u001b[0m, in \u001b[0;36mBaseFileLock.acquire\u001b[0;34m(self, timeout, poll_interval, poll_intervall, blocking)\u001b[0m\n\u001b[1;32m    342\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLock \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m not acquired on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, waiting \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m seconds ...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    343\u001b[0m         _LOGGER\u001b[38;5;241m.\u001b[39mdebug(msg, lock_id, lock_filename, poll_interval)\n\u001b[0;32m--> 344\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(poll_interval)\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# Something did go wrong, so decrement the counter.\u001b[39;00m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39mlock_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39mlock_counter \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_with_lora_and_hard_negatives(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    batch_size=batch_size,\n",
    "    num_samples=num_samples,\n",
    "    device=device,\n",
    "    num_epochs=10,\n",
    "    ckpt_path=\"models/best-lora.pt\",\n",
    "    grad_clip_norm=1.0,\n",
    "    scheduler=scheduler,\n",
    "    hard_negative_ratio=0.1,\n",
    "    hard_negative_update_freq=1,\n",
    "    use_amp=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

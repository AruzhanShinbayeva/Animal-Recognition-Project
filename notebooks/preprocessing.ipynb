{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import json\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "\n",
    "from typing import Any, Tuple, List\n",
    "\n",
    "from cv2 import Mat\n",
    "from numpy import dtype, floating, integer, ndarray\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 10)  # (w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/iwildcam2020_train_annotations.json\") as f:\n",
    "\tdata = json.load(f)\n",
    "\n",
    "\n",
    "annotations = pd.DataFrame.from_dict(data[\"annotations\"])\n",
    "images_metadata = pd.DataFrame.from_dict(data[\"images\"])\n",
    "categories = pd.DataFrame.from_dict(data[\"categories\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 217959 entries, 0 to 217958\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count   Dtype \n",
      "---  ------          --------------   ----- \n",
      " 0   seq_num_frames  217959 non-null  int64 \n",
      " 1   location        217959 non-null  int64 \n",
      " 2   datetime        217959 non-null  object\n",
      " 3   id              217959 non-null  object\n",
      " 4   frame_num       217959 non-null  int64 \n",
      " 5   seq_id          217959 non-null  object\n",
      " 6   width           217959 non-null  int64 \n",
      " 7   height          217959 non-null  int64 \n",
      " 8   file_name       217959 non-null  object\n",
      "dtypes: int64(5), object(4)\n",
      "memory usage: 15.0+ MB\n"
     ]
    }
   ],
   "source": [
    "images_metadata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datetime type and split into day/night time\n",
    "def split_day_night_time(\n",
    "\tdata: pd.DataFrame, day_start: str = \"06:00:00\", day_end: str = \"18:00:00\"\n",
    ") -> pd.DataFrame:\n",
    "\tdata = data.copy()\n",
    "\tdata[\"datetime\"] = pd.to_datetime(data[\"datetime\"])\n",
    "\tdata[\"is_day\"] = data[\"datetime\"].apply(\n",
    "\t\tlambda x: True\n",
    "\t\tif pd.Timestamp(day_start).time() <= x.time() < pd.Timestamp(day_end).time()\n",
    "\t\telse False\n",
    "\t)\n",
    "\treturn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dark_images(\n",
    "\timage: np.ndarray,\n",
    ") -> Mat | ndarray[Any, dtype[integer[Any] | floating[Any]]]:\n",
    "\timg = cv2.cvtColor(image, cv2.COLOR_RGB2LUV)\n",
    "\timg_eq = img.copy()\n",
    "\timg_eq[:, :, 0] = cv2.equalizeHist(img[:, :, 0])\n",
    "\tfinal_img = cv2.cvtColor(img_eq, cv2.COLOR_LUV2RGB)\n",
    "\treturn final_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "class iWildCam2020Dataset(IterableDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: str,\n",
    "        metadata: pd.DataFrame,\n",
    "        batch_size: int = 16,\n",
    "        resize_dim: Tuple[int, int] | None = None,\n",
    "        num_samples: int = 1000,\n",
    "        mean: np.ndarray | None = None,\n",
    "        std: np.ndarray | None = None,\n",
    "        save_dir: str | None = None,\n",
    "        overwrite: bool = False,\n",
    "        split: str = \"train\",\n",
    "        val_ratio: float = 0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.metadata = metadata\n",
    "\n",
    "        self.split = split\n",
    "        self.val_ratio = val_ratio\n",
    "        self.train_size = int((1 - val_ratio) * num_samples)\n",
    "        self.val_size = num_samples - self.train_size\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.resize_dim = resize_dim\n",
    "\n",
    "        self.num_samples = num_samples\n",
    "        if self.split == \"train\":\n",
    "            self.num_batches = (self.train_size + batch_size - 1) // batch_size\n",
    "        else:\n",
    "            self.num_batches= (self.val_size + batch_size - 1) // batch_size\n",
    "\n",
    "        self.mean = torch.tensor(mean if mean is not None else [0.0, 0.0, 0.0]).view(\n",
    "            3, 1, 1\n",
    "        )\n",
    "        self.std = torch.tensor(std if std is not None else [1.0, 1.0, 1.0]).view(\n",
    "            3, 1, 1\n",
    "        )\n",
    "\n",
    "        self.save_dir = Path(save_dir) if save_dir else None\n",
    "        if self.save_dir:\n",
    "            self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.overwrite = overwrite\n",
    "\n",
    "    def save_image(self, img_tensor: torch.Tensor, idx: int):\n",
    "        if self.save_dir:\n",
    "            save_path = self.save_dir / f\"image_{idx}.pt\"\n",
    "            torch.save(img_tensor, save_path)\n",
    "    \n",
    "    def load_image(self, idx: int) -> torch.Tensor | None:\n",
    "        if self.save_dir:\n",
    "            save_path = self.save_dir / f\"image_{idx}.pt\"\n",
    "            if save_path.exists():\n",
    "                return torch.load(save_path, weights_only=True)\n",
    "        return None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.split == \"train\":\n",
    "            start_idx, end_idx = 0, self.train_size\n",
    "        else:\n",
    "            start_idx, end_idx = self.train_size, self.num_samples\n",
    "        \n",
    "        for idx, image_batch in enumerate(self.dataset.iter(self.batch_size)):\n",
    "            # to get consistent part of dataset + val / train split\n",
    "            batch_start = idx * self.batch_size\n",
    "            if batch_start >= end_idx:\n",
    "                break\n",
    "            if batch_start < start_idx:\n",
    "                continue\n",
    "            \n",
    "            is_day = self.metadata[idx * self.batch_size : (idx + 1) * self.batch_size][\n",
    "                \"is_day\"\n",
    "            ].values\n",
    "            image_batch = image_batch[\"image\"]\n",
    "            imgs_ = []\n",
    "\n",
    "            dark_idx = set(np.where(~is_day)[0].tolist())\n",
    "            for i in range(len(image_batch)):\n",
    "                img_tensor = self.load_image(idx * self.batch_size + i)\n",
    "                if img_tensor is None:\n",
    "                    img = np.transpose(image_batch[i].numpy())\n",
    "                    if i in dark_idx:\n",
    "                        img = preprocess_dark_images(img)\n",
    "                    img = cv2.resize(img, self.resize_dim, interpolation=cv2.INTER_AREA)\n",
    "                    img_tensor = (\n",
    "                        torch.tensor(np.transpose(img, (2, 0, 1)), dtype=torch.float32)\n",
    "                        / 255.0\n",
    "                    )\n",
    "\n",
    "                    if self.save_dir:\n",
    "                        self.save_image(img_tensor, idx * self.batch_size + i)\n",
    "\n",
    "                imgs_.append(img_tensor)\n",
    "            yield torch.stack(imgs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_std(dataset, batch_size=32, resize_dim=(224, 224), num_samples=1000):\n",
    "    means = []\n",
    "    stds = []\n",
    "    for idx, image_batch in tqdm(enumerate(dataset.iter(batch_size)), total = ((num_samples + batch_size - 1) // batch_size)):\n",
    "        if idx * batch_size >= num_samples:\n",
    "            break\n",
    "\n",
    "        imgs_ = []\n",
    "        for image in image_batch[\"image\"]:\n",
    "            img = np.transpose(image.numpy(), (1, 2, 0))\n",
    "            img = cv2.resize(img, resize_dim, interpolation=cv2.INTER_AREA)\n",
    "            img = img / 255.0\n",
    "            imgs_.append(img)\n",
    "\n",
    "        imgs_array = np.stack(imgs_)\n",
    "        means.append(imgs_array.mean(axis=(0, 1, 2)))\n",
    "        stds.append(imgs_array.std(axis=(0, 1, 2)))\n",
    "\n",
    "    mean = np.mean(means, axis=0)\n",
    "    std = np.mean(stds, axis=0)\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_metadata = split_day_night_time(images_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea950f497f1e4bf0b953d18821e876a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75331b2c67134cf59bf235d780b7da5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc43ccc5ba243f6b72a6e24668e7a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b39fc03b454d4ea88c89eda9d23db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "\t\"anngrosha/iWildCam2020\", split=\"train\", streaming=True\n",
    ").with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "img_size = 528\n",
    "resize_dim = (img_size, img_size)\n",
    "\n",
    "num_samples = 200\n",
    "val_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d80ede855e974e0288968b4a0c46d813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(array([0.39645247, 0.40370792, 0.3785379 ]),\n",
       " array([0.25464274, 0.25719384, 0.26200439]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, std = calculate_mean_std(\n",
    "    dataset, batch_size=batch_size, resize_dim=resize_dim, num_samples=num_samples\n",
    ")\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 3, 528, 528])\n",
      "torch.Size([4, 3, 528, 528])\n",
      "torch.Size([4, 3, 528, 528])\n",
      "torch.Size([4, 3, 528, 528])\n",
      "torch.Size([4, 3, 528, 528])\n",
      "torch.Size([4, 3, 528, 528])\n",
      "torch.Size([4, 3, 528, 528])\n",
      "torch.Size([4, 3, 528, 528])\n",
      "torch.Size([4, 3, 528, 528])\n",
      "torch.Size([4, 3, 528, 528])\n",
      "torch.Size([4, 3, 528, 528])\n",
      "torch.Size([4, 3, 528, 528])\n",
      "torch.Size([4, 3, 528, 528])\n",
      "torch.Size([4, 3, 528, 528])\n",
      "torch.Size([4, 3, 528, 528])\n",
      "torch.Size([4, 3, 528, 528])\n",
      "torch.Size([4, 3, 528, 528])\n",
      "torch.Size([4, 3, 528, 528])\n",
      "torch.Size([4, 3, 528, 528])\n",
      "torch.Size([4, 3, 528, 528])\n",
      "torch.Size([4, 3, 528, 528])\n",
      "torch.Size([4, 3, 528, 528])\n",
      "torch.Size([4, 3, 528, 528])\n",
      "torch.Size([4, 3, 528, 528])\n",
      "torch.Size([4, 3, 528, 528])\n",
      "torch.Size([4, 3, 528, 528])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = iWildCam2020Dataset(\n",
    "    dataset=dataset,\n",
    "    metadata=images_metadata,\n",
    "    batch_size=batch_size,\n",
    "    resize_dim=resize_dim,\n",
    "    num_samples=num_samples,\n",
    "    mean=mean,\n",
    "    std=std,\n",
    "    save_dir=\"./data/train\",\n",
    "    split=\"train\",\n",
    "    val_ratio=val_ratio\n",
    ")\n",
    "\n",
    "val_dataset = iWildCam2020Dataset(\n",
    "    dataset=dataset,\n",
    "    metadata=images_metadata,\n",
    "    batch_size=batch_size,\n",
    "    resize_dim=resize_dim,\n",
    "    num_samples=num_samples,\n",
    "    mean=mean,\n",
    "    std=std,\n",
    "    save_dir=\"./data/val\",\n",
    "    split=\"val\",\n",
    "    val_ratio=val_ratio\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=None)\n",
    "val_loader = DataLoader(val_dataset, batch_size=None)\n",
    "\n",
    "for batch in train_loader:\n",
    "\tprint(batch.shape)\n",
    "\n",
    "for batch in val_loader:\n",
    "\tprint(batch.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

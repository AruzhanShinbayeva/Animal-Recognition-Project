{"cells":[{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T20:02:41.632675Z","iopub.status.busy":"2024-10-26T20:02:41.631694Z","iopub.status.idle":"2024-10-26T20:02:48.445312Z","shell.execute_reply":"2024-10-26T20:02:48.444299Z","shell.execute_reply.started":"2024-10-26T20:02:41.632607Z"},"trusted":true},"outputs":[],"source":["%matplotlib inline\n","import json\n","\n","import mlflow\n","import mlflow.pytorch\n","\n","import cv2\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import torch\n","import torch.nn as nn\n","from datasets import load_dataset\n","\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, IterableDataset\n","# from torchvision import transforms\n","# from torchvision import transforms\n","from sklearn.metrics import accuracy_score\n","\n","from typing import Any, Tuple, List\n","\n","from cv2 import Mat\n","from numpy import dtype, floating, integer, ndarray\n","\n","from tqdm.autonotebook import tqdm\n","\n","plt.rcParams[\"figure.figsize\"] = (16, 10)  # (w, h)"]},{"cell_type":"markdown","metadata":{},"source":["Run in terminal to set up MLFlow tracking server"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["#!mlflow server --host 127.0.0.1 --port 8080"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T20:02:48.448290Z","iopub.status.busy":"2024-10-26T20:02:48.447649Z","iopub.status.idle":"2024-10-26T20:02:51.463191Z","shell.execute_reply":"2024-10-26T20:02:51.462090Z","shell.execute_reply.started":"2024-10-26T20:02:48.448240Z"},"trusted":true},"outputs":[],"source":["with open(\"../data/iwildcam2020_train_annotations.json\") as f:\n","\tdata = json.load(f)\n","\n","annotations = pd.DataFrame.from_dict(data[\"annotations\"])\n","images_metadata = pd.DataFrame.from_dict(data[\"images\"])\n","categories = pd.DataFrame.from_dict(data[\"categories\"])"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T20:02:51.464772Z","iopub.status.busy":"2024-10-26T20:02:51.464441Z","iopub.status.idle":"2024-10-26T20:02:51.471045Z","shell.execute_reply":"2024-10-26T20:02:51.470040Z","shell.execute_reply.started":"2024-10-26T20:02:51.464738Z"},"trusted":true},"outputs":[],"source":["# convert datetime type and split into day/night time\n","def split_day_night_time(\n","    data: pd.DataFrame, day_start: str = \"06:00:00\", day_end: str = \"18:00:00\"\n",") -> pd.DataFrame:\n","    data = data.copy()\n","    data[\"datetime\"] = pd.to_datetime(data[\"datetime\"])\n","    data[\"is_day\"] = data[\"datetime\"].apply(\n","        lambda x: True\n","        if pd.Timestamp(day_start).time() <= x.time() < pd.Timestamp(day_end).time()\n","        else False\n","    )\n","    return data"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T20:02:51.473185Z","iopub.status.busy":"2024-10-26T20:02:51.472313Z","iopub.status.idle":"2024-10-26T20:02:51.483156Z","shell.execute_reply":"2024-10-26T20:02:51.482330Z","shell.execute_reply.started":"2024-10-26T20:02:51.473153Z"},"trusted":true},"outputs":[],"source":["def preprocess_dark_images(\n","    image: np.ndarray,\n",") -> Mat | ndarray[Any, dtype[integer[Any] | floating[Any]]]:\n","    img = cv2.cvtColor(image, cv2.COLOR_RGB2LUV)\n","    img_eq = img.copy()\n","    img_eq[:, :, 0] = cv2.equalizeHist(img[:, :, 0])\n","    final_img = cv2.cvtColor(img_eq, cv2.COLOR_LUV2RGB)\n","    return final_img"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T20:02:51.486324Z","iopub.status.busy":"2024-10-26T20:02:51.486037Z","iopub.status.idle":"2024-10-26T20:02:51.507189Z","shell.execute_reply":"2024-10-26T20:02:51.506240Z","shell.execute_reply.started":"2024-10-26T20:02:51.486295Z"},"trusted":true},"outputs":[],"source":["from pathlib import Path\n","\n","class iWildCam2020Dataset(IterableDataset):\n","    def __init__(\n","        self,\n","        dataset: str,\n","        metadata: pd.DataFrame,\n","        batch_size: int = 16,\n","        resize_dim: Tuple[int, int] | None = None,\n","        num_samples: int = 1000,\n","        mean: np.ndarray | None = None,\n","        std: np.ndarray | None = None,\n","        save_dir: str | None = None,\n","        overwrite: bool = False,\n","        split: str = \"train\",\n","        val_ratio: float = 0.2,\n","    ):\n","        super().__init__()\n","        self.metadata = metadata\n","\n","        self.split = split\n","        self.val_ratio = val_ratio\n","        self.train_size = int((1 - val_ratio) * num_samples)\n","        self.val_size = num_samples - self.train_size\n","\n","        self.dataset = dataset\n","        self.batch_size = batch_size\n","        self.resize_dim = resize_dim\n","\n","        self.num_samples = num_samples\n","        if self.split == \"train\":\n","            self.num_batches = (self.train_size + batch_size - 1) // batch_size\n","        else:\n","            self.num_batches= (self.val_size + batch_size - 1) // batch_size\n","\n","        self.mean = torch.tensor(mean if mean is not None else [0.0, 0.0, 0.0]).view(\n","            3, 1, 1\n","        )\n","        self.std = torch.tensor(std if std is not None else [1.0, 1.0, 1.0]).view(\n","            3, 1, 1\n","        )\n","\n","        self.save_dir = Path(save_dir) if save_dir else None\n","        if self.save_dir:\n","            self.save_dir.mkdir(parents=True, exist_ok=True)\n","        self.overwrite = overwrite\n","\n","    def save_image(self, img_tensor: torch.Tensor, idx: int):\n","        if self.save_dir:\n","            save_path = self.save_dir / f\"image_{idx}.pt\"\n","            torch.save(img_tensor, save_path)\n","    \n","    def load_image(self, idx: int) -> torch.Tensor | None:\n","        if self.save_dir:\n","            save_path = self.save_dir / f\"image_{idx}.pt\"\n","            if save_path.exists():\n","                return torch.load(save_path, weights_only=True)\n","        return None\n","    \n","    def __len__(self):\n","        return self.num_batches\n","\n","    def __iter__(self):\n","        if self.split == \"train\":\n","            start_idx, end_idx = 0, self.train_size\n","        else:\n","            start_idx, end_idx = self.train_size, self.num_samples\n","        \n","        for idx, image_batch in enumerate(self.dataset.iter(self.batch_size)):\n","            # to get consistent part of dataset + val / train split\n","            batch_start = idx * self.batch_size\n","            if batch_start >= end_idx:\n","                break\n","            if batch_start < start_idx:\n","                continue\n","            \n","            is_day = self.metadata[idx * self.batch_size : (idx + 1) * self.batch_size][\n","                \"is_day\"\n","            ].values\n","            image_batch = image_batch[\"image\"]\n","            imgs_ = []\n","\n","            dark_idx = set(np.where(~is_day)[0].tolist())\n","            for i in range(len(image_batch)):\n","                img_tensor = self.load_image(idx * self.batch_size + i)\n","                if img_tensor is None:\n","                    img = np.transpose(image_batch[i].numpy())\n","                    if i in dark_idx:\n","                        img = preprocess_dark_images(img)\n","                    img = cv2.resize(img, self.resize_dim, interpolation=cv2.INTER_AREA)\n","                    img_tensor = (\n","                        torch.tensor(np.transpose(img, (2, 0, 1)), dtype=torch.float32)\n","                        / 255.0\n","                    )\n","\n","                    if self.save_dir:\n","                        self.save_image(img_tensor, idx * self.batch_size + i)\n","\n","                imgs_.append(img_tensor)\n","            yield torch.stack(imgs_)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T20:02:51.508525Z","iopub.status.busy":"2024-10-26T20:02:51.508226Z","iopub.status.idle":"2024-10-26T20:02:51.521448Z","shell.execute_reply":"2024-10-26T20:02:51.520749Z","shell.execute_reply.started":"2024-10-26T20:02:51.508495Z"},"trusted":true},"outputs":[],"source":["def calculate_mean_std(dataset, batch_size=32, resize_dim=(224, 224), num_samples=1000):\n","    means = []\n","    stds = []\n","    for idx, image_batch in tqdm(enumerate(dataset.iter(batch_size)), total = ((num_samples + batch_size - 1) // batch_size)):\n","        if idx * batch_size >= num_samples:\n","            break\n","\n","        imgs_ = []\n","        for image in image_batch[\"image\"]:\n","            img = np.transpose(image.numpy(), (1, 2, 0))\n","            img = cv2.resize(img, resize_dim, interpolation=cv2.INTER_AREA)\n","            img = img / 255.0\n","            imgs_.append(img)\n","\n","        imgs_array = np.stack(imgs_)\n","        means.append(imgs_array.mean(axis=(0, 1, 2)))\n","        stds.append(imgs_array.std(axis=(0, 1, 2)))\n","\n","    mean = np.mean(means, axis=0)\n","    std = np.mean(stds, axis=0)\n","    return mean, std"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["import os\n","from datetime import datetime\n","\n","def get_unique_model_path(base_path):\n","    if not os.path.exists(base_path):\n","        return base_path\n","    \n","    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","    unique_path = f\"{base_path}_{timestamp}.pt\"\n","    \n","    while os.path.exists(unique_path):\n","        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","        unique_path = f\"{base_path}_{timestamp}.pt\"\n","    \n","    return unique_path"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T20:02:51.522990Z","iopub.status.busy":"2024-10-26T20:02:51.522626Z","iopub.status.idle":"2024-10-26T20:02:51.538479Z","shell.execute_reply":"2024-10-26T20:02:51.537618Z","shell.execute_reply.started":"2024-10-26T20:02:51.522949Z"},"trusted":true},"outputs":[],"source":["def train(\n","    model,\n","    criterion,\n","    optimizer,\n","    train_loader,\n","    val_loader,\n","    batch_size,\n","    device,\n","    num_epochs=1,\n","    ckpt_path=\"models/best.pt\"\n","):\n","    \n","    ckpt_path = get_unique_model_path(ckpt_path)\n","    best = 0.0\n","\n","    with mlflow.start_run():\n","        mlflow.log_param(\"model\", model)\n","        mlflow.log_param(\"criterion\", criterion)\n","        mlflow.log_param(\"optimizer\", optimizer)\n","        mlflow.log_param(\"model_path\", ckpt_path)\n","        mlflow.log_param(\"num_epochs\", num_epochs)\n","        mlflow.log_param(\"batch_size\", batch_size)\n","\n","        for epoch in range(num_epochs):\n","            train_loop = tqdm(\n","                enumerate(train_loader, 0),\n","                total=len(train_loader),\n","                desc=f\"Epoch {epoch}: train\",\n","            )\n","\n","            model.train()\n","            train_loss = 0.0\n","\n","            for i, batch in train_loop:\n","                images = batch.to(device)\n","                labels = torch.tensor(\n","                    annotations[\"category_id\"][\n","                        epoch * (len(train_loader) * batch_size) + batch_size * i : min(\n","                            epoch * (len(train_loader) * batch_size) + batch_size * (i + 1),\n","                            len(annotations[\"category_id\"]),\n","                        )\n","                    ].values\n","                ).to(device)\n","\n","                optimizer.zero_grad()\n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","                loss.backward()\n","                optimizer.step()\n","\n","                train_loss += loss.item()\n","                train_loop.set_postfix({\"loss\": loss.item()})\n","\n","            correct = 0\n","            total = 0\n","            val_loss = 0\n","            with torch.no_grad():\n","                model.eval()\n","\n","                val_loop = tqdm(\n","                    enumerate(val_loader, 0),\n","                    total=len(val_loader),\n","                    desc=f\"Val\",\n","                )\n","\n","                for i, batch in val_loop:\n","                    images = batch.to(device)\n","                    labels = torch.tensor(\n","                        annotations[\"category_id\"][\n","                            epoch * len(train_loader) * batch_size\n","                            + batch_size * i : min(\n","                                epoch * len(train_loader) * batch_size\n","                                + batch_size * (i + 1),\n","                                len(annotations[\"category_id\"]),\n","                            )\n","                        ].values\n","                    ).to(device)\n","\n","                    outputs = model(images)\n","                    _, predicted = torch.max(outputs, 1)\n","\n","                    loss = criterion(outputs, labels)\n","                    val_loss += loss.item()\n","\n","                    total += labels.size(0)\n","                    correct += (predicted == labels).sum().item()\n","                    val_loop.set_postfix({\"acc\": correct / total, \"loss\": val_loss / (i + 1)})\n","\n","\n","                val_accuracy = correct / total\n","\n","                mlflow.log_metric(\"train_loss\", train_loss / len(train_loader), step=epoch)\n","                mlflow.log_metric(\"val_accuracy\", val_accuracy, step=epoch)\n","                mlflow.log_metric(\"val_loss\", val_loss / len(val_loader), step=epoch)\n","                \n","                print(f\"Epoch [{epoch + 1}/{num_epochs}], Training Loss: {train_loss / len(train_loader):.6f}\")\n","                print(f\"Epoch [{epoch + 1}/{num_epochs}], Validation Accuracy: {val_accuracy:.6f}, Validation Loss: {val_loss:.6f}\")\n","                \n","                if val_accuracy > best:\n","                    best = val_accuracy\n","                    torch.save(model.state_dict(), ckpt_path)\n","                    mlflow.pytorch.log_model(model, f\"{ckpt_path[:-2]}\")"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T20:02:51.539888Z","iopub.status.busy":"2024-10-26T20:02:51.539529Z","iopub.status.idle":"2024-10-26T20:02:54.036238Z","shell.execute_reply":"2024-10-26T20:02:54.035426Z","shell.execute_reply.started":"2024-10-26T20:02:51.539856Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"686bcdd0c8e841e8ab18247962229aa7","version_major":2,"version_minor":0},"text/plain":["Resolving data files:   0%|          | 0/190 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"15d3a2f698ae4058a49a1d69ac2a8eb9","version_major":2,"version_minor":0},"text/plain":["Resolving data files:   0%|          | 0/52 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d4ec16a3e80343c5831af67d82562061","version_major":2,"version_minor":0},"text/plain":["Resolving data files:   0%|          | 0/190 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"646b3079d1af4266ad7ac42459966636","version_major":2,"version_minor":0},"text/plain":["Resolving data files:   0%|          | 0/52 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["dataset = load_dataset(\n","    \"anngrosha/iWildCam2020\", split=\"train\", streaming=True\n",").with_format(\"torch\")"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T20:02:54.037655Z","iopub.status.busy":"2024-10-26T20:02:54.037333Z","iopub.status.idle":"2024-10-26T20:03:12.673413Z","shell.execute_reply":"2024-10-26T20:03:12.672621Z","shell.execute_reply.started":"2024-10-26T20:02:54.037622Z"},"trusted":true},"outputs":[],"source":["images_metadata = split_day_night_time(images_metadata)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T20:21:23.236692Z","iopub.status.busy":"2024-10-26T20:21:23.236273Z","iopub.status.idle":"2024-10-26T20:21:23.270432Z","shell.execute_reply":"2024-10-26T20:21:23.269166Z","shell.execute_reply.started":"2024-10-26T20:21:23.236653Z"},"trusted":true},"outputs":[],"source":["batch_size = 5\n","img_size = 224\n","resize_dim = (img_size, img_size)\n","num_classes = max(annotations[\"category_id\"])\n","\n","num_samples = 5\n","val_ratio = 0\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{},"source":["Simple CNN"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class SimpleCNN(nn.Module):\n","    def __init__(self, num_classes):\n","        super(SimpleCNN, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n","        self.fc1 = nn.Linear(32 * 56 * 56, 128)\n","        self.fc2 = nn.Linear(128, num_classes)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = self.pool(self.relu(self.conv1(x)))\n","        x = self.pool(self.relu(self.conv2(x)))\n","        x = x.view(-1, 32 * 56 * 56)\n","        x = self.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T20:21:26.784221Z","iopub.status.busy":"2024-10-26T20:21:26.783840Z","iopub.status.idle":"2024-10-26T20:21:28.772839Z","shell.execute_reply":"2024-10-26T20:21:28.771846Z","shell.execute_reply.started":"2024-10-26T20:21:26.784184Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dc7a46fd37e1442eadc8920462cefbf2","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["(array([0.28920304, 0.30016854, 0.272847  ]),\n"," array([0.248005  , 0.25105431, 0.26049172]))"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["mean, std = calculate_mean_std(\n","    dataset, batch_size=batch_size, resize_dim=resize_dim, num_samples=num_samples\n",")\n","mean, std"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T20:21:29.250845Z","iopub.status.busy":"2024-10-26T20:21:29.250321Z","iopub.status.idle":"2024-10-26T20:21:29.258764Z","shell.execute_reply":"2024-10-26T20:21:29.257691Z","shell.execute_reply.started":"2024-10-26T20:21:29.250803Z"},"trusted":true},"outputs":[],"source":["train_dataset = iWildCam2020Dataset(\n","    dataset=dataset,\n","    metadata=images_metadata,\n","    batch_size=batch_size,\n","    resize_dim=resize_dim,\n","    num_samples=num_samples,\n","    mean=mean,\n","    std=std,\n","    save_dir=\"/working/data/train\",\n","    split=\"train\",\n","    val_ratio=val_ratio\n",")\n","\n","val_dataset = iWildCam2020Dataset(\n","    dataset=dataset,\n","    metadata=images_metadata,\n","    batch_size=batch_size,\n","    resize_dim=resize_dim,\n","    num_samples=num_samples,\n","    mean=mean,\n","    std=std,\n","    save_dir=\"/working/data/val\",\n","    split=\"val\",\n","    val_ratio=val_ratio\n",")\n","\n","\n","train_loader = DataLoader(train_dataset, batch_size=None)\n","val_loader = DataLoader(val_dataset, batch_size=None)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T20:24:25.525364Z","iopub.status.busy":"2024-10-26T20:24:25.524527Z","iopub.status.idle":"2024-10-26T20:24:25.533990Z","shell.execute_reply":"2024-10-26T20:24:25.532915Z","shell.execute_reply.started":"2024-10-26T20:24:25.525324Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\anngr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _torch_pytree._register_pytree_node(\n"]}],"source":["model = SimpleCNN(num_classes)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adadelta(model.parameters(), lr=1)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["# train(\n","#     model,\n","#     criterion,\n","#     optimizer,\n","#     batch_loader,\n","#     #val_loader,\n","#     batch_loader,\n","#     batch_size,\n","#     device,\n","#     num_epochs=20\n","# )"]},{"cell_type":"markdown","metadata":{},"source":["Performing singe-batch overfitting to see if model capable enought for our task"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T20:24:27.064065Z","iopub.status.busy":"2024-10-26T20:24:27.063092Z","iopub.status.idle":"2024-10-26T20:25:51.940672Z","shell.execute_reply":"2024-10-26T20:25:51.939609Z","shell.execute_reply.started":"2024-10-26T20:24:27.064023Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aef2090dfd42448f8dfe9d70ab984049","version_major":2,"version_minor":0},"text/plain":["Epoch 0: train:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"238e9b3bb5f244d9a64c0aeda2fd5b3f","version_major":2,"version_minor":0},"text/plain":["Val:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch [1/3], Training Loss: 5.809134\n","Epoch [1/3], Validation Accuracy: 0.200000, Validation Loss: 4.822841\n"]},{"name":"stderr","output_type":"stream","text":["2024/10/27 16:08:16 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8e41691eece9468dbe6148c396e8ae84","version_major":2,"version_minor":0},"text/plain":["Epoch 1: train:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7c50275b390e4dacba2dd79b5b355ae7","version_major":2,"version_minor":0},"text/plain":["Val:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch [2/3], Training Loss: 4.814448\n","Epoch [2/3], Validation Accuracy: 0.600000, Validation Loss: 2.278366\n"]},{"name":"stderr","output_type":"stream","text":["2024/10/27 16:10:57 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"26f36cd40663428ba9de3df4747cc9ff","version_major":2,"version_minor":0},"text/plain":["Epoch 2: train:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4eb78e6e72164914963cfa1316ad2e84","version_major":2,"version_minor":0},"text/plain":["Val:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2024/10/27 16:13:19 INFO mlflow.tracking._tracking_service.client: 🏃 View run treasured-mole-198 at: http://127.0.0.1:8080/#/experiments/0/runs/f0807d87eb5f41b092fcb449651bbfd3.\n","2024/10/27 16:13:19 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:8080/#/experiments/0.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [3/3], Training Loss: 4.153296\n","Epoch [3/3], Validation Accuracy: 0.600000, Validation Loss: 1.631622\n"]}],"source":["batch_dataset = iWildCam2020Dataset(\n","    dataset=dataset,\n","    metadata=images_metadata,\n","    batch_size=batch_size,\n","    resize_dim=resize_dim,\n","    num_samples=num_samples,\n","    mean=mean,\n","    std=std,\n","    save_dir=\"/working/data/train\",\n","    split=\"train\",\n","    val_ratio=0\n",")\n","batch_loader = DataLoader(batch_dataset, batch_size=None)\n","\n","train(\n","    model,\n","    criterion,\n","    optimizer,\n","    batch_loader,\n","    batch_loader,\n","    batch_size,\n","    device,\n","    num_epochs=3,\n","    ckpt_path='models/simpleCNN.pt'\n",")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5954512,"sourceId":9730298,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"}},"nbformat":4,"nbformat_minor":4}

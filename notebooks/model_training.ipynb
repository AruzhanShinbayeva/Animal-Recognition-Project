{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import json\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "# from torchvision import transforms\n",
    "# from torchvision.models import resnet18\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from typing import Any, Tuple, List\n",
    "\n",
    "from cv2 import Mat\n",
    "from numpy import dtype, floating, integer, ndarray\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 10)  # (w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/iwildcam2020_train_annotations.json\") as f:\n",
    "\tdata = json.load(f)\n",
    "\n",
    "\n",
    "annotations = pd.DataFrame.from_dict(data[\"annotations\"])\n",
    "images_metadata = pd.DataFrame.from_dict(data[\"images\"])\n",
    "categories = pd.DataFrame.from_dict(data[\"categories\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 217959 entries, 0 to 217958\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count   Dtype \n",
      "---  ------          --------------   ----- \n",
      " 0   seq_num_frames  217959 non-null  int64 \n",
      " 1   location        217959 non-null  int64 \n",
      " 2   datetime        217959 non-null  object\n",
      " 3   id              217959 non-null  object\n",
      " 4   frame_num       217959 non-null  int64 \n",
      " 5   seq_id          217959 non-null  object\n",
      " 6   width           217959 non-null  int64 \n",
      " 7   height          217959 non-null  int64 \n",
      " 8   file_name       217959 non-null  object\n",
      "dtypes: int64(5), object(4)\n",
      "memory usage: 15.0+ MB\n"
     ]
    }
   ],
   "source": [
    "images_metadata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datetime type and split into day/night time\n",
    "def split_day_night_time(\n",
    "\tdata: pd.DataFrame, day_start: str = \"06:00:00\", day_end: str = \"18:00:00\"\n",
    ") -> pd.DataFrame:\n",
    "\tdata = data.copy()\n",
    "\tdata[\"datetime\"] = pd.to_datetime(data[\"datetime\"])\n",
    "\tdata[\"is_day\"] = data[\"datetime\"].apply(\n",
    "\t\tlambda x: True\n",
    "\t\tif pd.Timestamp(day_start).time() <= x.time() < pd.Timestamp(day_end).time()\n",
    "\t\telse False\n",
    "\t)\n",
    "\treturn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dark_images(\n",
    "\timage: np.ndarray,\n",
    ") -> Mat | ndarray[Any, dtype[integer[Any] | floating[Any]]]:\n",
    "\timg = cv2.cvtColor(image, cv2.COLOR_RGB2LUV)\n",
    "\timg_eq = img.copy()\n",
    "\timg_eq[:, :, 0] = cv2.equalizeHist(img[:, :, 0])\n",
    "\tfinal_img = cv2.cvtColor(img_eq, cv2.COLOR_LUV2RGB)\n",
    "\treturn final_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "class iWildCam2020Dataset(IterableDataset):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tdataset: str,\n",
    "\t\tmetadata: pd.DataFrame,\n",
    "\t\tbatch_size: int = 16,\n",
    "\t\tresize_dim: Tuple[int, int] | None = None,\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.metadata = metadata\n",
    "\n",
    "\t\tself.dataset = dataset\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.resize_dim = resize_dim\n",
    "\n",
    "\tdef __iter__(self):  # -> Generator[Any, Any, None]:\n",
    "\t\tfor idx, image_batch in enumerate(self.dataset.iter(self.batch_size)):\n",
    "\t\t\tis_day = self.metadata[idx : idx + self.batch_size][\"is_day\"].values\n",
    "\t\t\timage_batch = image_batch[\"image\"]\n",
    "\t\t\timgs_ = []\n",
    "\t\t\t\n",
    "\t\t\tdark_idx = set(np.where(~is_day)[0].tolist())\n",
    "\t\t\tfor i in range(len(image_batch)):\n",
    "\t\t\t\timg = np.transpose(image_batch[i].numpy())\n",
    "\t\t\t\tif i in dark_idx:\n",
    "\t\t\t\t\timg = preprocess_dark_images(img)\n",
    "\t\t\t\timg = cv2.resize(img, self.resize_dim, interpolation=cv2.INTER_AREA)\n",
    "\t\t\t\t# imgs_.append(torch.tensor(np.transpose(img)))\n",
    "\t\t\t\timgs_.append(torch.tensor(np.transpose(img, (2, 0, 1)), dtype=torch.float32) / 255.0)\n",
    "\t\t\tyield torch.stack(imgs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 56 * 56, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 56 * 56)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "def train(\n",
    "    model, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    dataset_iterable, \n",
    "    batch_size,\n",
    "    num_epochs=1,\n",
    "    train_batches=5,\n",
    "    val_batches=2,\n",
    "    ckpt_path=\"best.pt\",\n",
    "):\n",
    "    best = 0.0\n",
    "    epoch_items = (train_batches + val_batches) * batch_size\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        train_loop = tqdm(\n",
    "            enumerate(dataset_iterable, 0),\n",
    "            total=train_batches,\n",
    "            desc=f\"Epoch {epoch}: train\",\n",
    "        )\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for i, batch in train_loop:\n",
    "            images = batch\n",
    "            labels = torch.tensor(annotations['category_id'][epoch * epoch_items + batch_size * i : min(epoch * epoch_items + batch_size * (i + 1), len(annotations['category_id']))].values)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_loop.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "            if i + 1 > train_batches:\n",
    "                break\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss / (batch_size * train_batches):.4f}')\n",
    "\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            val_loop = tqdm(\n",
    "                enumerate(dataset_iterable, 0),\n",
    "                total=val_batches,\n",
    "                desc=f\"Val\",\n",
    "            )\n",
    "            \n",
    "            for i, batch in val_loop:\n",
    "                images = batch\n",
    "                labels = torch.tensor(annotations['category_id'][epoch * epoch_items + batch_size * train_batches + batch_size * i : min(epoch * epoch_items + batch_size * train_batches + batch_size * (i + 1), len(annotations['category_id']))].values)\n",
    "\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                val_loop.set_postfix({\"acc\": correct / total})\n",
    "\n",
    "                if i + 1 > val_batches:\n",
    "                    break\n",
    "                \n",
    "\n",
    "            if correct / total > best:\n",
    "                torch.save(model.state_dict(), ckpt_path)\n",
    "                best = correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb0ae979d1764cc48e377a271e0d219f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fca616a521c43e4be1481553be007be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ac5e6fbe6dc4b618051baed5c4eedc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60da70d1f13c45ffa043a8922f6607b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "\t\"anngrosha/iWildCam2020\", split=\"train\", streaming=True\n",
    ").with_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_metadata = split_day_night_time(images_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "img_size = 224\n",
    "num_classes = max(annotations['category_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_iterable = iWildCam2020Dataset(\n",
    "\tdataset=dataset,\n",
    "\tmetadata=images_metadata,\n",
    "\tbatch_size=batch_size,\n",
    "\tresize_dim=(img_size, img_size),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN(num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4470575c1fe54cafa9cfe9cc71584f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0: train:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Loss: 0.4057\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8045e7278804524aef0d6ebf0ca1a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064bcd481bdd4d25b40c1f968fd7a622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1: train:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2], Loss: 0.5088\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f27032da8de448e59f65f0b6338999ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(\n",
    "    model, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    dataset_iterable, \n",
    "    batch_size,\n",
    "    num_epochs=2,\n",
    "    train_batches=5,\n",
    "    val_batches=2,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
